{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d21410-16ef-467c-97e8-a58b30dfb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting\n",
    "# - https://github.com/ollama/ollama/blob/main/docs/import.md\n",
    "# Resources:\n",
    "# - https://medium.com/@danushidk507/rag-with-llama-using-ollama-a-deep-dive-into-retrieval-augmented-generation-c58b9a1cfcd3\n",
    "# - https://github.com/ollama/ollama/blob/main/docs/linux.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13544143-5bd6-4f1c-878f-1ba1145da1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to run `ollama`\n",
    "# From terminal with same conda env activated as in this notebook:\n",
    "# curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
    "# tar -xzf ollama-linux-amd64.tgz\n",
    "## -> you'll now have bin/ and lib/. \n",
    "## -> I extracted the contents of each to /path/to/conda/env/{bin,lib}/ollama\n",
    "## -> created /path/to/conda/env/etc/conda/activate.d/ollama.sh to prepend the corresponding paths to $PATH and $LD_LIBRARY_PATH (exported obviously) \n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += \":/research/rgs01/home/clusterHome/jpastr08/.conda/envs/py310/bin/ollama/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d73ecef-258d-48d0-a760-a142ffccab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpastr08/.conda/envs/py310/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf4f9a4-08b1-4d4f-90ca-3241889b25f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commented out to avoid running every time\n",
    "#!pip install langchain\n",
    "#!pip install -U langchain-community\n",
    "#!pip install sentence-transformers\n",
    "#!pip install faiss-gpu\n",
    "#!pip install pypdf\n",
    "#!pip install langchain_ollama\n",
    "#!pip install colab-xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0be5f8-0537-45ae-8c07-a4712a171f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load the document\n",
    "loader = PyPDFLoader(\"files/4-2_manual.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=750, chunk_overlap=100, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b74df6-4469-44d1-b27d-b065c9c2b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_844363/3121078218.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/jpastr08/.conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Embedding and Storage\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS # library for vector similarity search\n",
    "\n",
    "# Load embedding model \n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\" # known for its robust performance across various text tasks\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save and reload the vector store\n",
    "vectorstore.save_local(\"faiss_index_\")\n",
    "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = persisted_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef499bef-48f5-416e-b767-e8e1c72799b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From terminal with same conda env activated as in this notebook:\n",
    "# ollama serve & ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fa8bb8-2642-424b-8fdf-ac92d83bf97d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA model\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize the LLaMA model\n",
    "llm = Ollama(model=\"llama3.1\")\n",
    "\n",
    "# Test with a sample prompt\n",
    "response = llm.invoke(\"Tell me a joke\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dead0d-2e59-48b1-90e3-8d1313bb34d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " If the DRAGEN software is run with the parameter --enable-variant-caller=true, what are the implications? Construct a command line including this parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_844363/2730196570.py:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the DRAGEN software is run with the parameter `--enable-variant-caller=true`, it implies that the variant caller component will be enabled in the workflow. This means that DRAGEN will construct a workflow that includes the variant caller component and automatically resolve any component inconsistencies.\n",
      "\n",
      "Here's an example command line including this parameter:\n",
      "\n",
      "`dragen --enable-variant-caller=true --input-input-fastq.gz your_data.fastq.gz`\n",
      "\n",
      "Note: The `--input-input-fastq.gz` option is not specified in the provided text, but it is a general input option that DRAGEN uses to read input data. You should replace `your_data.fastq.gz` with your actual input file path.\n",
      "\n",
      "When the variant caller component is enabled, DRAGEN will produce its own set of VCFs and metric output files for each run.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " Using the Illumina DRAGEN software, how do you construct a command line for mapping FASTQ files to a reference genome?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example of how to construct a command line for mapping FASTQ files to a reference genome using the Illumina DRAGEN software:\n",
      "\n",
      "`dragen -r <reference_path> -1 <FASTQ_file_1> -2 <FASTQ_file_2> --RGID <Read_group_ID> --RGSMD <Sample_Name> --enable-map-align true`\n",
      "\n",
      "Where:\n",
      "- `<reference_path>` is the path to the reference genome\n",
      "- `<FASTQ_file_1>` and `<FASTQ_file_2>` are the paths to the two FASTQ files to be mapped\n",
      "- `<Read_group_ID>` is a unique identifier for the read group\n",
      "- `<Sample_Name>` is the name of the sample being analyzed\n",
      "\n",
      "For example:\n",
      "\n",
      "`dragen -r /staging/human/reference/hg38_alt_aware/DRAGEN/8 -1 /staging/test/data/NA12878_R1.fastq -2 /staging/test/data/NA12878_R2.fastq --RGID DRAGEN_RGID --RGSMD NA12878 --enable-map-align true`\n",
      "\n",
      "Note that you will need to replace the placeholders (`<reference_path>`, `<FASTQ_file_1>`, etc.) with your actual file paths and names.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " Provide an example using the reference genome GRCh38 and input files sample1_R1.fastq and sample1_R2.fastq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context only contains examples that use the hg19 reference genome, but it does not include any information on how to use the GRCh38 reference genome with the specified input files (sample1_R1.fastq and sample1_R2.fastq). \n",
      "\n",
      "However, based on the general usage of DRAGEN mentioned in the documentation, a possible command would be:\n",
      "\n",
      "```\n",
      "dragen-f \\\n",
      "-r /staging/human/reference/GRCh38/GRCh38.fa.k_21.f_16.m_149\\\n",
      "-1 sample1_R1.fastq.gz\\\n",
      "-2 sample1_R2.fastq.gz\\\n",
      "--output-directory/staging/examples/\\\n",
      "--output-file-prefixsample1_dragen\n",
      "```\n",
      "\n",
      "Note that you need to replace the path to the reference genome with your own location, and adjust the output directory and file prefix as needed. Additionally, you may need to specify other options depending on your specific use case.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " What is the command line syntax to enable variant calling in DRAGEN?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, I don't have information about how to specifically enable variant calling for DRAGEN. However, based on a general section that mentions configuring VARIANT CALLERs, it seems like the relevant option is:\n",
      "\n",
      "â€¢ Configure the VARIANT CALLERs based on the application\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " Include parameters for setting the output directory to /data/output and specifying a memory limit of 16 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the command line options you would need:\n",
      "\n",
      "`--output-directory Yes Specifies the output directory.` should be specified with `/data/output`.\n",
      "\n",
      "Unfortunately, there is no option directly mentioned in the provided context for \"specifying a memory limit\". However, there is an option to specify the number of threads (`-n --num-threads No Specifies the number of processor threads to use.`) which could indirectly influence the system's ability to handle memory.\n",
      "\n",
      "Therefore, you would add these two options to your command:\n",
      "\n",
      "```\n",
      "dragen \\\n",
      "--output-directory /data/output \\\n",
      "-n 16\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " Write a command line to perform mapping, variant calling, and generate a BAM file, using DRAGEN. Include the input files reads_R1.fastq and reads_R2.fastq, a reference genome hg19, and an output directory /output_dir\u000b",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the command line to perform mapping, variant calling, and generate a BAM file using DRAGEN:\n",
      "\n",
      "```bash\n",
      "dragen \\\n",
      "-r /path/to/reference/hg19 \\\n",
      "--fastq-file1 /path/to/input/reads_R1.fastq \\\n",
      "--fastq-file2 /path/to/input/reads_R2.fastq \\\n",
      "--output-directory /output_dir \\\n",
      "--output-file-prefix BAM_file_prefix \\\n",
      "--RGID DRAGEN_RGID \\\n",
      "--RGSM sample_name \\\n",
      "--enable-map-align=true \\\n",
      "--enable-cyp21a2=true\n",
      "```\n",
      "\n",
      "Note that you should replace `/path/to/reference/hg19` with the actual path to your reference genome, and `sample_name` with a meaningful name for your sample. Also, make sure to adjust the output file prefix as needed.\n",
      "\n",
      "This command line uses FASTQ input files (`reads_R1.fastq` and `reads_R2.fastq`) and maps them against the hg19 reference genome using DRAGEN's mapper/aligner. The resulting BAM file will be stored in the `/output_dir` directory with a name specified by the `--output-file-prefix` option.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type your query (or type 'Exit' to quit): \n",
      " The following command results in an error: dragen --input sample.fastq --output-dir /output. Correct the command and explain the changes you made.\u000b",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original command is incorrect because it does not specify a reference directory, which is required by DRAGEN.\n",
      "\n",
      "Here's the corrected command:\n",
      "```\n",
      "dragen \\\n",
      "-r /staging/human/reference/hg19/hg19.fa.k_21.f_16.m_149 \\\n",
      "--input sample.fastq \\\n",
      "--output-dir /output\n",
      "```\n",
      "The changes made were:\n",
      "\n",
      "* I added the reference directory option (`-r`) to specify a valid reference directory. This is required for DRAGEN to function correctly.\n",
      "* The `--fastq-file1` and `--fastq-file2` options are not necessary in this case, since only one input file (sample.fastq) is provided. These options are typically used when working with paired-end FASTQ files.\n",
      "* I removed the `--RGID` and `--RGSM` options, as they are typically required for passing a FASTQ file as input, but not necessary in this case.\n",
      "* The rest of the command remains unchanged.\n",
      "\n",
      "Note that without more information about the specific version of DRAGEN being used or the specific reference directory available, it's difficult to provide a completely accurate answer.\n"
     ]
    }
   ],
   "source": [
    "# Use RAG with retriever defined above\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# Interactive query loop\n",
    "while True:\n",
    "    query = input(\"Type your query (or type 'Exit' to quit): \\n\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    result = qa.run(query)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071d559b-261b-47f7-94f6-13f40395dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code graveyard\n",
    "\n",
    "## Instead of the following (to have venvs within a conda env), I think I could just launch the \n",
    "## jupyter notebook from each venv with jupyter installed in it. Doing that should take the venv as\n",
    "## the environment jupyter is in, which in turn would also be within the conda env. \n",
    "\n",
    "# From a terminal window, after activating the same conda environment this notebook runs in.\n",
    "# cd <dir of choice, probably within the scope of the notebook>\n",
    "# python -m venv venv_rag_langchain\n",
    "# source myenv/bin/activate\n",
    "# pip install ipykernel\n",
    "# python -m ipykernel install --user --name=venv_rag_langchain\n",
    "\n",
    "# If you want to remove the kernel\n",
    "# jupyter kernelspec remove venv_rag_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f36a7f-48d0-41f0-917d-5122b732e663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
