{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba14fae-95cf-4ba7-8097-eb9add14c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API docs\n",
    "# - https://platform.openai.com/docs/api-reference\n",
    "# file_search:\n",
    "# - https://platform.openai.com/docs/assistants/tools/file-search?context=without-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23563665-52ea-4274-b103-48e0c6e120db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpastr08/.conda/envs/py310/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2913f51-cd26-4c6b-b3ca-893a49fd0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.conda/envs/py310/bin/python\n",
      "~/.conda/envs/py310/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4c42c0-a0af-4b8e-a944-7857b5a01b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commented out to be able to `Run All Cells` smoothly.\n",
    "\n",
    "#!pip install PyPDF2\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a978e2-7caf-4218-bedf-ae276b87a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\n",
      "1.57.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import openai\n",
    "\n",
    "print(sys.version)\n",
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1935a19b-2768-409f-8eba-b08e07093831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\"oai_token.txt\", 'r') as file:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = file.readline().strip()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f014ac6-373c-4738-a76c-de52faf593f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to interact with PDF docs\n",
    "\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "def load_document(pdf_document_filepath):\n",
    "    # Load the PDF file\n",
    "    reader = PdfReader(pdf_document_filepath)\n",
    "    \n",
    "    # Get the number of pages\n",
    "    global NUM_PAGES\n",
    "    NUM_PAGES = len(reader.pages)\n",
    "    print(f\"Number of pages: {NUM_PAGES}\")\n",
    "\n",
    "# pages is a 0-indexed array\n",
    "def extract_pages(input_pdf, output_pdf, pages):\n",
    "    reader = PdfReader(input_pdf)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    for page_num in pages:\n",
    "        writer.add_page(reader.pages[page_num])\n",
    "\n",
    "    with open(output_pdf, 'wb') as output_file:\n",
    "        writer.write(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4558b5d-55e4-48f9-9d58-79d6fb17eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to interact with OpenAI API\n",
    "\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    " \n",
    "client = OpenAI()\n",
    "\n",
    "# These are the assistants to send messages to and (they will return the prompts with which to populate the dataset).\n",
    "def create_dragen_manual_assistant():\n",
    "    global assistant, assistant_id\n",
    "    \n",
    "    assistant = client.beta.assistants.create(\n",
    "      name=\"DRAGEN Assistant\",\n",
    "      instructions=\"You are an expert reader of manual pages, which include text, tables, and images. Use the documents provided to you to answer questions about what the manual says. You must stick to the manual and not complement your responses with any information not included in the manual pages provided.\",\n",
    "      model=\"gpt-4o\",\n",
    "      tools=[{\"type\": \"file_search\"}],\n",
    "    )\n",
    "    assistant_id = assistant.id\n",
    "        \n",
    "# Start a thread with a message containing a file.\n",
    "async def create_thread_with_message(filepath, message, prev_filepath=None):\n",
    "    client = AsyncOpenAI()\n",
    "    messages=[{\"role\": \"user\",\"content\": message,\"attachments\": []}]\n",
    "\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        # Upload the file\n",
    "        message_file = await client.files.create(\n",
    "            file=file, purpose=\"assistants\"\n",
    "        )\n",
    "        # Attach the new file to the message.\n",
    "        messages[0][\"attachments\"].append(\n",
    "            {\n",
    "                \"file_id\": message_file.id, \n",
    "                \"tools\": [{\"type\": \"file_search\"}]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Handle prev page\n",
    "    if prev_filepath:\n",
    "        with open(prev_filepath, \"rb\") as prev:\n",
    "            # Upload the file\n",
    "            prev_message_file = await client.files.create(\n",
    "                file=prev, purpose=\"assistants\"\n",
    "            )\n",
    "            # Attach the new file to the message.\n",
    "            messages[0][\"attachments\"].append(\n",
    "                {\n",
    "                    \"file_id\": prev_message_file.id, \n",
    "                    \"tools\": [{\"type\": \"file_search\"}]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "    # Create a thread and attach the files to the message\n",
    "    thread = await client.beta.threads.create(messages=messages)\n",
    "    \n",
    "    thread_id = thread.id \n",
    "    #file_id = message_file.id\n",
    "    #vector_store_id = thread.tool_resources.file_search.vector_store_ids[0]\n",
    "    #prev_vector_store_id = prev_vector_store.id\n",
    "    return thread_id #, file_id, vector_store_id, prev_vector_store_id\n",
    "\n",
    "# Use the create-and-poll SDK helper to create a run and poll its status\n",
    "# until it's in a terminal state.\n",
    "async def create_and_submit_run(thread_id):\n",
    "    client = AsyncOpenAI()\n",
    "    run = await client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread_id, assistant_id=assistant_id\n",
    "    )\n",
    "    \n",
    "    messages = []\n",
    "    async for message in client.beta.threads.messages.list(thread_id=thread_id, run_id=run.id):\n",
    "        messages.append(message)\n",
    "    return messages\n",
    "\n",
    "def extract_response(messages):    \n",
    "    message_content = messages[0].content[0].text\n",
    "    annotations = message_content.annotations\n",
    "    citations = []\n",
    "    for index, annotation in enumerate(annotations):\n",
    "        message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "        if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "            cited_file = client.files.retrieve(file_citation.file_id)\n",
    "            citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "    \n",
    "    response = message_content.value\n",
    "    references = \"\\n\".join(citations)\n",
    "\n",
    "    return response, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b74ea4-cefb-4ea9-b212-286b227b4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages/instructions to submit to LLM/assistant to process documents into prompts\n",
    "\n",
    "manual_message = \"\"\"You are a document processing assistant tasked with generating prompts based on the content of a manual page. \n",
    "You are being provided with two pages: the current manual page and the preceding page for context. Focus primarily on the current page. Refer to the preceding page only when necessary to ensure continuity for sections or information that began earlier. The page number is located on the lower right corner of every page.\n",
    "\n",
    "Follow these instructions carefully:\n",
    "1. Analyze the current page from the manual for relevant information, including text, tables, diagrams, and examples. Relevant information refers to useful information for someone who is trying to learn how to use this platform. \n",
    "2. Generate between 10 and 20 detailed Alpaca-format prompts based on the relevance and length of the current (not the preceding) page.\n",
    "3. Ensure that the response is formatted in the Alpaca structure, which is a JSON array of objects. Each object represents a single prompt and contains the following fields:\n",
    "instruction: The task or query for the model (required).\n",
    "input: Any context or details relevant to the instruction (optional).\n",
    "output: The model's expected response (required).\n",
    "system: The system instruction, which should always be \"Do not add information not explicitly stated or speculate.\".\n",
    "history: A list of relevant prior [instruction, response] pairs, representing conversational history (optional).\n",
    "Each field should be enclosed in quotation marks, separated by commas, and formatted as valid JSON. Multiple prompts should be enclosed in square brackets [] to form an array.\n",
    "4. Include exactly one prompt that explicitly states the current page number as the source of the information being discussed.\n",
    "5. If the page includes examples or diagrams, generate prompts that specifically cover those examples and diagrams.\n",
    "6. Some pages may contain information that continues from a previous page. If this is the case, refer to the previous page to ensure the prompts comprehensively address the complete context of the information.\n",
    "7. Include all important details from the page in the prompts. The prompts do not have to be concise but must be thorough, comprehensive, and factual.\n",
    "8. Do not wrap your response in a type label such as ```json```\n",
    "\n",
    "Use these guidelines to process the provided page and return the output in valid Alpaca format. Only return the prompts, nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb86baa4-2cda-4e89-aab8-16b2d12fe653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "# main coroutine\n",
    "async def event_loop(pages_to_process, message_to_assistant):\n",
    "    prev_filepath = None\n",
    "    # for cleaning up later\n",
    "    tasks = {}\n",
    "    thread_ids = deque([])\n",
    "    pages = deque([])\n",
    "    for page in pages_to_process:\n",
    "        print(f\"\\tProcessing page {page}\")\n",
    "                \n",
    "        if page > 6: # page 6 is the first content page\n",
    "            prev_filepath = f\"files/page_{page-1}.pdf\"\n",
    "            if not os.path.exists(prev_filepath):\n",
    "                extract_pages(pdf_document_filepath, prev_filepath, [page-1])\n",
    "\n",
    "        page_filepath = f\"files/page_{page}.pdf\"\n",
    "        if not os.path.exists(page_filepath):\n",
    "            extract_pages(pdf_document_filepath, page_filepath, [page])\n",
    "        \n",
    "        thread_id = await create_thread_with_message(page_filepath, message_to_assistant, prev_filepath=prev_filepath)\n",
    "\n",
    "        task = create_and_submit_run(thread_id)\n",
    "        tasks[str(page)] = task\n",
    "\n",
    "        # for cleaning up later\n",
    "        thread_ids.append(thread_id)\n",
    "        pages.append(page)\n",
    "        \n",
    "    # Wait for all tasks to complete\n",
    "    raw_responses = await asyncio.gather(*tasks.values())\n",
    "    return raw_responses, tasks, thread_ids, pages\n",
    "\n",
    "def save_results(raw_responses_dict, overwrite=False):\n",
    "    stored_prompts = {}\n",
    "    with open('files/metadata.json', 'r') as metadata:\n",
    "        stored_prompts = json.load(metadata)\n",
    "    \n",
    "    new_prompts = {}\n",
    "    for page, raw_response in raw_responses_dict.items():\n",
    "        if page not in stored_prompts or overwrite:\n",
    "            new_prompts[page] = extract_response(raw_response)[0]            \n",
    "    \n",
    "    stored_prompts.update(new_prompts)\n",
    "    \n",
    "    with open('files/metadata.json', 'w') as metadata:\n",
    "        json.dump(stored_prompts, metadata, indent=4)  # 'indent=4' makes the JSON file pretty-printed\n",
    "\n",
    "# to clean up threads and tasks\n",
    "async def clean_up(tasks, thread_ids, pages):\n",
    "    client = AsyncOpenAI()\n",
    "    while thread_ids:\n",
    "        t_id = thread_ids.popleft()\n",
    "        try:\n",
    "            await client.beta.threads.delete(t_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Cleaning up threads failed: {e}\")\n",
    "\n",
    "    # Not necessary unless global\n",
    "    #while pages:\n",
    "    #    p = pages.popleft()\n",
    "    #    del(tasks[p])\n",
    "\n",
    "async def submitter(pages_to_process, message_to_assistant, overwrite=False):\n",
    "    raw_responses, tasks, thread_ids, pages_processed = await event_loop(pages_to_process, message_to_assistant)\n",
    "\n",
    "    raw_responses_dict = {page: response for page, response in zip(tasks.keys(), raw_responses)}\n",
    "    save_results(raw_responses_dict, overwrite)\n",
    "    \n",
    "    await clean_up(tasks, thread_ids, pages_processed)    \n",
    "\n",
    "    return pages_processed\n",
    "\n",
    "async def driver(message_to_assistant, pages_to_process=[]): # first 6 pages and last page are non-content\n",
    "    if pages_to_process:\n",
    "        await submitter(pages_to_process, message_to_assistant, overwrite=True)\n",
    "    else:\n",
    "        stored_prompts = {}\n",
    "        with open('files/metadata.json', 'r') as metadata:\n",
    "            stored_prompts = json.load(metadata)\n",
    "    \n",
    "        for page in range(6, NUM_PAGES-1, 10): \n",
    "            if str(page) in stored_prompts:\n",
    "                continue\n",
    "\n",
    "            # batching calls to event_loop for fault tolerance\n",
    "            [pages_to_process.append(p) for p in range(page, page+10)]\n",
    "            if pages_to_process[-1] >= NUM_PAGES-1:\n",
    "                break\n",
    "                \n",
    "            pages_processed = await submitter(pages_to_process, message_to_assistant)\n",
    "            # for check at start of the loop, since event_loop process more than 1 page at a time\n",
    "            for page in pages_processed:\n",
    "                stored_prompts[str(page)] = None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe27c9f-f10e-451b-ab22-ef23d7859f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reprocess failed responses\n",
    "def reprocess_failed():\n",
    "    stored_prompts = {}\n",
    "    with open('files/metadata.json', 'r') as metadata:\n",
    "        stored_prompts = json.load(metadata)\n",
    "    \n",
    "    pages_to_reprocess = []\n",
    "    for page, prompt_arr in stored_prompts.items():\n",
    "        if isinstance(prompt_arr, str):\n",
    "            try:\n",
    "                isinstance(json.loads(prompt_arr), list)\n",
    "            except Exception:\n",
    "                #print(f\"Will reprocess page {page}\")\n",
    "                #print(prompt_arr)\n",
    "                pages_to_reprocess.append(int(page))\n",
    "        else:\n",
    "            print(f\"ERROR: value not stored as a string but a: {type(prompt_arr)}.\")\n",
    "\n",
    "    return pages_to_reprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e24a42f-091a-46c3-b8a9-1681bbc0e3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 740\n"
     ]
    }
   ],
   "source": [
    "# cell driving main logic\n",
    "\n",
    "load_document(\"files/4-2_manual.pdf\")\n",
    "create_dragen_manual_assistant()\n",
    "await driver(manual_message)\n",
    "failed = reprocess_failed()\n",
    "while failed:\n",
    "    await driver(manual_message, pages_to_process=failed)\n",
    "    failed = reprocess_failed()\n",
    "    print(f\"reprocessing {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74103953-ed5b-40e1-8e53-f9db8d4fb190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finalize data for fine-tuning step in next notebook\n",
    "stored_prompts = {}\n",
    "with open('files/metadata.json', 'r') as file:\n",
    "    stored_prompts = json.load(file)\n",
    "\n",
    "# for training, i think the length of the history arrays must be the same.\n",
    "#history_len = 2 # i got this number manually by checking the max length of the history arrays\n",
    "\n",
    "all_prompts = []\n",
    "for p, page_arr in stored_prompts.items():\n",
    "    if isinstance(page_arr, str):  # check if `arr` is still a JSON string\n",
    "        page_arr = json.loads(page_arr) # ensure each value is parsed into a Python object\n",
    "    for prompt_dict in page_arr:\n",
    "        if \"input\" not in prompt_dict or not isinstance(prompt_dict[\"input\"], str):\n",
    "            prompt_dict[\"input\"] = \"\"\n",
    "        if \"history\" not in prompt_dict:\n",
    "            prompt_dict[\"history\"] = []\n",
    "        #else:\n",
    "        #    if prompt_dict[\"history\"]: # arr is not empty   \n",
    "        #        while len(prompt_dict[\"history\"]) < history_len:\n",
    "        #            prompt_dict[\"history\"].append([])\n",
    "            \n",
    "    all_prompts.extend(page_arr)  # Use `extend` for nested lists\n",
    "\n",
    "with open('files/dataset.json', 'w') as file:\n",
    "    json.dump(all_prompts, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a26e065-190d-464b-8871-93d530c20bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep in mind that entries in dataset.json are prompts, while entries in metadata.json are pages with sets of prompts. Although they are both in the same order, if you're trying to map from one to the other this becomes relevant.\n",
      "\n",
      "Entry 16 contains a non empty history array of size 1\n",
      "Entry 1286 contains a non empty history array of size 2\n",
      "Entry 1515 contains a non empty history array of size 1\n",
      "Entry 2127 contains a non empty history array of size 2\n",
      "Entry 2629 contains a non empty history array of size 1\n",
      "Entry 2799 contains a non empty history array of size 1\n",
      "Entry 3026 contains a non empty history array of size 1\n",
      "Entry 3410 contains a non empty history array of size 1\n",
      "Entry 4113 contains a non empty history array of size 1\n",
      "Entry 4320 contains a non empty history array of size 1\n",
      "Entry 4326 contains a non empty history array of size 1\n",
      "Entry 4488 contains a non empty history array of size 1\n",
      "Entry 4489 contains a non empty history array of size 1\n",
      "Entry 5738 contains a non empty history array of size 1\n",
      "Entry 5739 contains a non empty history array of size 1\n",
      "Entry 6480 contains a non empty history array of size 1\n",
      "Validation completed.\n"
     ]
    }
   ],
   "source": [
    "# just validating the json schema\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"files/dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Keep in mind that entries in dataset.json are prompts, while entries in metadata.json\", \n",
    "      \"are pages with sets of prompts. Although they are both in the same order, if you're trying\",\n",
    "      \"to map from one to the other this becomes relevant.\\n\")\n",
    "\n",
    "for idx, dict_ in enumerate(data):\n",
    "    if not isinstance(dict_, dict):\n",
    "        print(f\"Entry {idx} is not a dictionary: {dict_}\")\n",
    "        continue\n",
    "    if \"instruction\" not in dict_.keys():\n",
    "        print(f\"Entry {idx} does not contain 'instruction' key.\")\n",
    "    if \"input\" not in dict_.keys():\n",
    "        print(f\"Entry {idx} does not contain 'input' key.\")\n",
    "    if \"output\" not in dict_.keys():\n",
    "        print(f\"Entry {idx} does not contain 'output' key.\")\n",
    "    if \"system\" not in dict_.keys():\n",
    "        print(f\"Entry {idx} does not contain 'system' key.\")\n",
    "    if \"history\" not in dict_.keys():\n",
    "        print(f\"Entry {idx} does not contain 'history' key.\")\n",
    "        print(dict_)\n",
    "        break #continue\n",
    "    if not isinstance(dict_[\"instruction\"], str):\n",
    "        print(f\"Entry {idx} has non-str 'instruction': {dict_['instruction']}\")\n",
    "    if not isinstance(dict_[\"input\"], str):\n",
    "        print(f\"Entry {idx} has non-str 'input': {dict_['input']}\")\n",
    "    if not isinstance(dict_[\"output\"], str):\n",
    "        print(f\"Entry {idx} has non-str 'output': {dict_['output']}\")\n",
    "    if not isinstance(dict_[\"system\"], str):\n",
    "        print(f\"Entry {idx} has non-str 'system': {dict_['system']}\")\n",
    "    if not isinstance(dict_[\"history\"], list):\n",
    "        print(f\"Entry {idx} has non-list 'history': {dict_['history']}\")\n",
    "        continue\n",
    "    if dict_[\"history\"]:\n",
    "        print(f\"Entry {idx} contains a non empty history array of size\", len(dict_[\"history\"]))\n",
    "        for history_arr in dict_[\"history\"]:\n",
    "            #print(f\"\\t{history_arr}\")\n",
    "            if not isinstance(history_arr, list):\n",
    "                print(f\"Entry {idx} has non-list 'history' item: {history_arr}\")\n",
    "            if \"instruction\" in history_arr or \"response\" in history_arr:\n",
    "                print(f\"Entry {idx} has invalid syntax that must be addressed: \\n\\t{history_arr}\")\n",
    "                \n",
    "print(\"Validation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d99080e2-4b64-40c3-a49d-77279a5c77b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files with purpose 'assistants' have been deleted.\n",
      "All returned vector stores have been deleted\n"
     ]
    }
   ],
   "source": [
    "# cleaning up\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.files.list(purpose=\"assistants\")\n",
    "for file in response.data:\n",
    "    client.files.delete(file.id)\n",
    "print(\"All files with purpose 'assistants' have been deleted.\")\n",
    "\n",
    "#retries = 0\n",
    "#while retries < 15:\n",
    "try:\n",
    "    vector_stores = client.beta.vector_stores.list()\n",
    "    #retries = 0\n",
    "    for vs in vector_stores:\n",
    "        try:\n",
    "            client.beta.vector_stores.delete(vs.id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting vector store: {vs}\")\n",
    "    print(\"All returned vector stores have been deleted\")\n",
    "except:\n",
    "    print(\"there was an issue, retrying...\")\n",
    "    #retries += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10b7176a-a999-41a6-a468-8535b6584337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: files/page_5.pdf\n",
      "Files renamed successfully!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1514307-7470-4794-969e-930646030188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code graveyard\n",
    "#    ### Handle prev page ###\n",
    "#    page_num = int(filepath.strip(\"files/page_.pdf\"))\n",
    "#    # Create a vector store to contain the preceding manual page.\n",
    "#   if page_num > 0:\n",
    "#        prev_vector_store = await client.beta.vector_stores.create(name=\"Preceding manual page\")\n",
    "#\n",
    "#        file_paths = [f\"files/page_{page_num-1}.pdf\"]\n",
    "#        file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "#        file_batch = await client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "#          vector_store_id=prev_vector_store.id, files=file_streams\n",
    "#        )\n",
    "#        if file_batch.status != \"completed\":\n",
    "#            print(f\"Upload failed for preceding file: {file_path}.\")\n",
    "#\n",
    "#        assistant = await client.beta.assistants.update(\n",
    "#          assistant_id=assistant_id,\n",
    "#          tool_resources={\"file_search\": {\"vector_store_ids\": [prev_vector_store.id]}},\n",
    "#        )\n",
    "#    ### End handle prev page ###\n",
    "\n",
    "\n",
    "#    - IMPORTANT: If the current page is a non-content page (such as a title page, table of contents, blank page, etc.) do not generate any prompts. Instead, return an empty response.\n",
    "\n",
    "# In progress:\n",
    "#\n",
    "#dragen_manual_message = \"\"\"You are a document processing assistant tasked with generating prompts based on the content of a manual page. \n",
    "#You are being provided with two pages: the current manual page and the preceding page for context. Focus primarily on the current page. Refer to the preceding page only when necessary to ensure continuity for sections or information that began earlier. The page number is located on the lower right corner of every page.\n",
    "#\n",
    "#Follow these instructions carefully:\n",
    "#1. Analyze the current page from the manual for relevant information, including text, tables, diagrams, and examples. Relevant information refers to useful information for someone who is trying to learn how to use this platform. \n",
    "#2. Generate about 100 detailed prompt based on the relevance and length of the current (not the preceding) page. It is expected that some prompts will cover the same content, although rephrased.\n",
    "#3. Every prompt should be an object in a JSON array. Follow the following structure for each one of these objects: \n",
    "#{\n",
    "#\"instruction\": \"human instruction to the model (required)\",\n",
    "#\"input\": \"context or details relevant to the instruction (optional)\",\n",
    "#\"output\": \"model response (required)\",\n",
    "#\"system\": \"the system instruction, which should always be 'Do not speculate or add information not explicitly stated.'\",\n",
    "#\"history\": [\n",
    "#  [\"human instruction in the first round\", \"model response in the first round\"],\n",
    "#  [\"human instruction in the second round\", \"model response in the second round\"],\n",
    "#  ...\n",
    "#]\n",
    "#}\n",
    "#Each field should be enclosed in quotation marks, separated by commas, and formatted as valid JSON. Multiple prompts should be enclosed in square brackets [] to form an array.\n",
    "#4. Include exactly one prompt that explicitly states the current page number as the source of the information being discussed.\n",
    "#5. If the page includes examples or diagrams, generate prompts that specifically cover those examples and diagrams.\n",
    "#6. Some pages may contain information that continues from a previous page. If this is the case, refer to the previous page to ensure the prompts comprehensively address the complete context of the information.\n",
    "#7. Include all important details from the page in the prompts. The prompts do not have to be concise but must be thorough, comprehensive, and factual.\n",
    "#8. Do not wrap your response in a type label such as ```json```\n",
    "#\n",
    "#Use these guidelines to process the provided page and return the output in valid Alpaca format. Only return the prompts, nothing else.\n",
    "#\"\"\"\n",
    "#\n",
    "#emails_message = \"\"\"You are a document processing assistant tasked with generating LLM fine-tuning prompts based on the content of an email thread.\n",
    "#You are being provided with an email thread spanning several email exchanges. The document provided to you includes the emails in chronological order. Emails include text, images, code, and URLs.\n",
    "#\n",
    "#Follow these instructions carefully:\n",
    "#1. Carefully analyze every page of the document for relevant information arising from the discussion. \n",
    "#2. Generate 150-250 detailed prompts summarizing what was learned in the exchanges, namely the solutions to the problems raised.\n",
    "#3. Devote sufficient prompts toward describing command lines discussed in the email. Understanding how to form these command lines correctly is of utmost importance, as examples are scant. However, any prompts devoted to code should stick to verifiable information and be 100% based on what is discussed in the email thread.\n",
    "#- IMPORTANT: do not invent any component of the command lines when creating the prompts.\n",
    "#4. Devote enough prompts to describe the pi\n",
    "#5. Do not take into consideration any personal information in the emails, such as names or email addresses.\n",
    "#6. Ensure that the response is formatted in the Alpaca structure, which is a JSON array of objects. Each object represents a single prompt. This is its structure:\n",
    "#{\n",
    "#\"instruction\": \"human instruction to the model (required)\",\n",
    "#\"input\": \"context or details relevant to the instruction (optional)\",\n",
    "#\"output\": \"model response (required)\",\n",
    "#\"system\": \"the system instruction, which should always be 'Do not speculate or add information not explicitly stated.'\",\n",
    "#\"history\": [\n",
    "#  [\"human instruction in the first round\", \"model response in the first round\"],\n",
    "#  [\"human instruction in the second round\", \"model response in the second round\"],\n",
    "#  ...\n",
    "#]\n",
    "#\"images\": [\n",
    "#  \"files/3.jpg\"\n",
    "#]\n",
    "#}\n",
    "#Note the following regarding the JSON object structure:\n",
    "#- the \"history\" array is optional and contains arrays with two strings, a prompt and a response. If used, it can have any length necessary.\n",
    "#- the \"images\" array is optional\n",
    "#\n",
    "#7. While you cannot access URLs, you can devote prompts to \n",
    "#8. Note that the document should be read page by page, and that the information of earlier pages affects the information on response pages, which is the most important. \n",
    "#9. If the content is not technical, ignore it\n",
    "#8. Do not wrap your response in a type label such as ```json```\n",
    "#6. Pay special attention to samples' data, whether written or images, which may be in a page of their own following the email they belong to.\n",
    "#\n",
    "#\n",
    "#Use these guidelines to process the provided document and return the output in valid Alpaca format. Only return the prompts, nothing else.\n",
    "#\"\"\"\n",
    "\n",
    "## Regularizing file names\n",
    "#\n",
    "#import os\n",
    "#\n",
    "#directory = \"files\"\n",
    "#\n",
    "#for i in range(0, 100):\n",
    "#    old_name = os.path.join(directory, f\"page_{i}.pdf\")\n",
    "#    new_name = os.path.join(directory, f\"page_{i:03}.pdf\") # Add leading zeros\n",
    "#\n",
    "#    if os.path.exists(old_name):\n",
    "#        os.rename(old_name, new_name)\n",
    "#    else:\n",
    "#        print(f\"File not found: {old_name}\")\n",
    "#\n",
    "#print(\"Files renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3adf17-0832-4e3f-9d66-c9f7b6f17860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
