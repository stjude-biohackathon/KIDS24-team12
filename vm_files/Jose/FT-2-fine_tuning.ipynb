{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5210ff-0e4e-4fbe-afff-5c532dd81cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from LLama Factory\n",
    "# https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n",
    "# Data formats\n",
    "# https://github.com/hiyouga/LLaMA-Factory/tree/main/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990fd0e6-3096-4534-bef8-eba473956bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.conda/envs/py310/bin/python\n",
      "~/.conda/envs/py310/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f9816e-2fe6-4311-bca7-805adf051965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --depth 1 https://github.com/PepeRulo/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3322c8-7b08-442b-848a-506be03735d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpastr08/.conda/envs/py310/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602f609f-b9ae-4e1e-8e53-727d71b0042b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commented out to be able to `Run All Cells` smoothly.\n",
    "\n",
    "#!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "#!pip uninstall -y jax\n",
    "#!pip install -e .[torch,bitsandbytes,liger-kernel]\n",
    "#!pip install huggingface_hub\n",
    "\n",
    "#!pip freeze > actual_requirements.txt\n",
    "#!cat actual_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9881c7-1515-402b-aeff-45f24ba5b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\n",
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba1a141-4893-459d-8af3-855c51b0074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 1507.35 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the total RAM in bytes\n",
    "total_ram = psutil.virtual_memory().total\n",
    "\n",
    "# Convert bytes to GB\n",
    "total_ram_gb = total_ram / (1024 ** 3)\n",
    "\n",
    "print(f\"Total RAM: {total_ram_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc2cbd3-c945-4082-ab95-481a10a63675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 8\n",
      "Current CUDA device: 0\n",
      "CUDA capability: (8, 0)\n",
      "2.3.1+cu121\n",
      "12.1\n",
      "Tue Dec 10 04:56:07 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              63W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              65W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:48:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              65W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              64W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              64W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              68W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              66W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 00000000:CB:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              62W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Get the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(device)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "\n",
    "# You can also get more details\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"CUDA capability: {torch.cuda.get_device_capability(device)}\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727c83d4-32ce-4464-853c-eaf80e1203ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpastr08/.conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "with open(\"../hf_token.txt\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2a1127-f911-436e-a0db-99fdaaeec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to run this when there's been updates to the prompt dataset!\n",
    "%cp /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/files/dataset.json \\\n",
    "    ./data/dragen_alpaca.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0feb25ad-0377-4e27-a8d6-50a68110fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/dataset_info.json\", \"r\") as f:\n",
    "    datasets = json.load(f)\n",
    "    \n",
    "# Per https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md#alpaca-format\n",
    "if \"dragen_alpaca\" not in datasets:\n",
    "    datasets[\"dragen_alpaca\"] = {\n",
    "        \"file_name\": \"dragen_alpaca.json\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\",\n",
    "            \"system\": \"system\",\n",
    "            \"history\": \"history\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"data/dataset_info.json\", \"w\") as f:\n",
    "        json.dump(datasets, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54b0c37-d8e8-4dca-bf9e-fa7306d73f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run in terminal on the same node this OnDemand session is on.\n",
    "# ssh <ondemand gpu node>\n",
    "# module load conda3/202402\n",
    "# conda activate <env>\n",
    "## To run Gradio board (which by default launches on localhost:7860)\n",
    "# cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n",
    "# llamafactory-cli webui\n",
    "## Run locally\n",
    "# ssh -L <port shown above>:localhost:<port shown above> local_port_forwarding_test\n",
    "## Open a browser locally to interface with Gradio board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642d553f-be3d-4276-b1b5-d7d37e8cc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in terminal within same conda env as here.\n",
    "# conda install nvidia/label/cuda-12.3.2::cuda-toolkit\n",
    "# pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c4e87e-78ea-4862-b6b4-bd1ec840db55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training on Gradio board \n",
    "# See in:\n",
    "# KIDS24-team12/vm_files/Jose/LLaMA-Factory/saves/Llama-3.1-70B-Instruct/lora/16_batch_size\n",
    "# KIDS24-team12/vm_files/Jose/LLaMA-Factory/saves/Llama-3.1-70B-Instruct/lora/8_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d2d7426-0f28-40a5-ac53-8d1d37af91ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:679] 2024-12-10 05:17:07,708 >> loading configuration file config.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-10 05:17:07,709 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:07,805 >> loading file tokenizer.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:07,805 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:07,805 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:07,806 >> loading file special_tokens_map.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:07,806 >> loading file tokenizer_config.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2475] 2024-12-10 05:17:08,066 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:679] 2024-12-10 05:17:08,462 >> loading configuration file config.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-10 05:17:08,463 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:08,558 >> loading file tokenizer.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:08,558 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:08,558 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:08,559 >> loading file special_tokens_map.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2024-12-10 05:17:08,559 >> loading file tokenizer_config.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-10 05:17:08,816 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-10 05:17:08] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n",
      "[INFO|2024-12-10 05:17:08] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:679] 2024-12-10 05:17:08,921 >> loading configuration file config.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-10 05:17:08,922 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-10 05:17:08] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2024-12-10 05:17:08] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3937] 2024-12-10 05:17:08,927 >> loading weights file model.safetensors from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-12-10 05:17:08,936 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2024-12-10 05:17:08,937 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2024-12-10 05:17:59,908 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-12-10 05:17:59,909 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-70B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2024-12-10 05:18:00,006 >> loading configuration file generation_config.json from cache at /home/jpastr08/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/snapshots/945c8663693130f8be2ee66210e062158b2a9693/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-12-10 05:18:00,007 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-10 05:18:00] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-12-10 05:18:03] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-70B-Instruct/lora/16_batch_size\n",
      "[INFO|2024-12-10 05:18:03] llamafactory.model.loader:157 >> all params: 70,605,479,936\n"
     ]
    }
   ],
   "source": [
    "# Infer base model with fine-tuned adapters\n",
    "\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",           # base model\n",
    "  adapter_name_or_path=\"saves/Llama-3.1-70B-Instruct/lora/16_batch_size\", # path to the LoRA adapters that were fine-tuned\n",
    "  template=\"llama3\",                                                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                                                # same to the one in training\n",
    "  quantization_bit=4,                                                    # load 4-bit quantized model\n",
    ")\n",
    "chat_model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ad6e7-caee-4414-bf0e-a23b2844ae4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  If the DRAGEN software is run with the parameter --enable-variant-caller=true, what are the implications? Construct a command line including this parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: When the DRAGEN software is run with the parameter --enable-variant-caller=true, it enables the variant caller, which is necessary for identifying genetic variants in the data. A command line with this parameter could look like: \n",
      "\n",
      "dragen -r /staging/human/reference/hg19/hg19.fa.k_21.f_16.m_149 \\\n",
      "--intermediate-results-dir /staging/examples/ \\\n",
      "--output-directory /staging/examples/ \\\n",
      "--output-file-prefix NA12878_e10_50M_30x_e10_50M \\\n",
      "--enable-map-align false \\\n",
      "--enable-map-align-output false \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true \\\n",
      "--enable-variant-caller true\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  Using the Illumina DRAGEN software, how do you construct a command line for mapping FASTQ files to a reference genome?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To map FASTQ files to a reference genome, the command line would include options like --ref-dir for the reference directory, --fastq-file1 and --fastq-file2 for the input FASTQ files, and --output-directory for the output location.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  Provide an example using the reference genome GRCh38 and input files sample1_R1.fastq and sample1_R2.fastq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: An example command line for mapping FASTQ files to a reference genome would be: \n",
      "\n",
      "dragen -r /staging/human/reference/hg38/hg38.fa.k_21.f_16.m_149 \\\n",
      "--fastq-file1 /staging/examples/reads/SRA056922_30x_e10_50M_1.fastq \\\n",
      "--fastq-file2 /staging/examples/reads/SRA056922_30x_e10_50M_2.fastq \\\n",
      "--output-directory /staging/examples/ \\\n",
      "--output-file-prefix NA12878_e10_50M_30x_e10_50M \\\n",
      "--enable-map-align true \\\n",
      "--enable-map-align-output true \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  What is the command line syntax to enable variant calling in DRAGEN?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To enable variant calling in DRAGEN, use the command line: \n",
      "\n",
      "dragen -r /staging/human/reference/hg38/hg38.fa.k_21.f_16.m_149 \\\n",
      "--fastq-file1 /staging/examples/reads/SRA056922_30x_e10_50M_1.fastq \\\n",
      "--fastq-file2 /staging/examples/reads/SRA056922_30x_e10_50M_2.fastq \\\n",
      "--output-directory /staging/examples/ \\\n",
      "--output-file-prefix NA12878_e10_50M_30x_e10_50M \\\n",
      "--enable-map-align true \\\n",
      "--enable-map-align-output true \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true \\\n",
      "--enable-variant-caller true\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  Include parameters for setting the output directory to /data/output and specifying a memory limit of 16 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To include parameters for setting the output directory to /data/output and specifying a memory limit of 16 GB, the command line would be: \n",
      "\n",
      "dragen -r /staging/human/reference/hg38/hg38.fa.k_21.f_16.m_149 \\\n",
      "--fastq-file1 /staging/examples/reads/SRA056922_30x_e10_50M_1.fastq \\\n",
      "--fastq-file2 /staging/examples/reads/SRA056922_30x_e10_50M_2.fastq \\\n",
      "--output-directory /data/output \\\n",
      "--output-file-prefix NA12878_e10_50M_30x_e10_50M \\\n",
      "--enable-map-align true \\\n",
      "--enable-map-align-output true \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true \\\n",
      "--enable-variant-caller true \\\n",
      "--enable-map-align-output true \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true \\\n",
      "--enable-variant-caller true \\\n",
      "--enable-map-align-output true \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true \\\n",
      "--enable-variant-caller true\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  Write a command line to perform mapping, variant calling, and generate a BAM file, using DRAGEN. Include the input files reads_R1.fastq and reads_R2.fastq, a reference genome hg19, and an output directory /output_dir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The command line to perform mapping, variant calling, and generate a BAM file, using DRAGEN, is:\n",
      "\n",
      "dragen -r /staging/human/reference/hg19/hg19.fa.k_21.f_16.m_149 \\\n",
      "--fastq-file1 /staging/examples/reads/reads_R1.fastq \\\n",
      "--fastq-file2 /staging/examples/reads/reads_R2.fastq \\\n",
      "--output-directory /output_dir \\\n",
      "--output-file-prefix NA12878_e10_50M_30x_e10_50M \\\n",
      "--enable-map-align true \\\n",
      "--enable-map-align-output true \\\n",
      "--enable-sort true \\\n",
      "--enable-duplicate-marking true \\\n",
      "--enable-variant-caller true\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  The following command results in an error: dragen --input sample.fastq --output-dir /output. Correct the command and explain the changes you made.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The corrected command is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"History has been removed.\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27565857-a18d-4ead-b189-356bbe96cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources:\n",
    "# https://www.reddit.com/r/LocalLLaMA/comments/1fps1cp/llama32_vs_llama31_in_medical_domain_llama31_70b/\n",
    "# https://github.com/NVIDIA/RULER?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a10353-2f7b-4543-854d-fd203afff43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
