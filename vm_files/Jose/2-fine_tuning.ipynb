{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5210ff-0e4e-4fbe-afff-5c532dd81cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from LLama Factory\n",
    "# https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing#scrollTo=CS0Qk5OR0i4Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990fd0e6-3096-4534-bef8-eba473956bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.conda/envs/py38/bin/python\n",
      "~/.conda/envs/py38/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3322c8-7b08-442b-848a-506be03735d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "602f609f-b9ae-4e1e-8e53-727d71b0042b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commented out to be able to `Run All Cells` smoothly.\n",
    "\n",
    "#!git clone --depth 1 https://github.com/PepeRulo/LLaMA-Factory.git\n",
    "\n",
    "#!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "#!pip uninstall -y jax\n",
    "#!pip install -e .[torch,bitsandbytes,liger-kernel]\n",
    "#!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a9881c7-1515-402b-aeff-45f24ba5b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.20 | packaged by conda-forge | (default, Sep 30 2024, 17:52:49) \n",
      "[GCC 13.3.0]\n",
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ba1a141-4893-459d-8af3-855c51b0074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 1507.35 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the total RAM in bytes\n",
    "total_ram = psutil.virtual_memory().total\n",
    "\n",
    "# Convert bytes to GB\n",
    "total_ram_gb = total_ram / (1024 ** 3)\n",
    "\n",
    "print(f\"Total RAM: {total_ram_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc2cbd3-c945-4082-ab95-481a10a63675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 4\n",
      "Current CUDA device: 0\n",
      "CUDA capability: (8, 0)\n",
      "2.3.1+cu121\n",
      "12.1\n",
      "Thu Dec  5 20:16:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              65W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:48:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              63W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              63W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              65W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Get the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(device)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "\n",
    "# You can also get more details\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"CUDA capability: {torch.cuda.get_device_capability(device)}\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727c83d4-32ce-4464-853c-eaf80e1203ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "with open(\"hf_token.txt\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a315c70e-2856-4662-b8d5-92afc5fdaa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e2a1127-f911-436e-a0db-99fdaaeec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/files/dataset.json \\\n",
    "    ./data/dragen_alpaca.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0feb25ad-0377-4e27-a8d6-50a68110fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/dataset_info.json\", \"r\") as f:\n",
    "    datasets = json.load(f)\n",
    "    \n",
    "# Per https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md#alpaca-format\n",
    "if \"dragen_alpaca\" not in datasets:\n",
    "    datasets[\"dragen_alpaca\"] = {\n",
    "        \"file_name\": \"dragen_alpaca.json\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\",\n",
    "            \"system\": \"system\",\n",
    "            \"history\": \"history\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"data/dataset_info.json\", \"w\") as f:\n",
    "        json.dump(datasets, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27565857-a18d-4ead-b189-356bbe96cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources:\n",
    "# https://www.reddit.com/r/LocalLLaMA/comments/1fps1cp/llama32_vs_llama31_in_medical_domain_llama31_70b/\n",
    "# https://github.com/NVIDIA/RULER?tab=readme-ov-file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
