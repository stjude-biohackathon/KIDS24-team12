{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5210ff-0e4e-4fbe-afff-5c532dd81cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from LLama Factory\n",
    "# https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing#scrollTo=CS0Qk5OR0i4Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990fd0e6-3096-4534-bef8-eba473956bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.conda/envs/py310/bin/python\n",
      "~/.conda/envs/py310/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f9816e-2fe6-4311-bca7-805adf051965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --depth 1 https://github.com/PepeRulo/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3322c8-7b08-442b-848a-506be03735d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/LLaMA-Factory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602f609f-b9ae-4e1e-8e53-727d71b0042b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commented out to be able to `Run All Cells` smoothly.\n",
    "\n",
    "#!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "#!pip uninstall -y jax\n",
    "#!pip install -e .[torch,bitsandbytes,liger-kernel]\n",
    "#!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9881c7-1515-402b-aeff-45f24ba5b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\n",
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba1a141-4893-459d-8af3-855c51b0074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 1507.35 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the total RAM in bytes\n",
    "total_ram = psutil.virtual_memory().total\n",
    "\n",
    "# Convert bytes to GB\n",
    "total_ram_gb = total_ram / (1024 ** 3)\n",
    "\n",
    "print(f\"Total RAM: {total_ram_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc2cbd3-c945-4082-ab95-481a10a63675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Number of GPUs: 4\n",
      "Current CUDA device: 0\n",
      "CUDA capability: (8, 0)\n",
      "2.3.1+cu121\n",
      "12.1\n",
      "Sun Dec  8 19:04:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              63W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              63W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              68W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:CB:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              62W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Get the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(device)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "\n",
    "# You can also get more details\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"CUDA capability: {torch.cuda.get_device_capability(device)}\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "727c83d4-32ce-4464-853c-eaf80e1203ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "with open(\"../hf_token.txt\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e2a1127-f911-436e-a0db-99fdaaeec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to run this when there's been updates to the prompt dataset!\n",
    "%cp /research/rgs01/home/clusterHome/jpastr08/biohackathon/KIDS24-team12/vm_files/Jose/files/dataset.json \\\n",
    "    ./data/dragen_alpaca.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0feb25ad-0377-4e27-a8d6-50a68110fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/dataset_info.json\", \"r\") as f:\n",
    "    datasets = json.load(f)\n",
    "    \n",
    "# Per https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md#alpaca-format\n",
    "if \"dragen_alpaca\" not in datasets:\n",
    "    datasets[\"dragen_alpaca\"] = {\n",
    "        \"file_name\": \"dragen_alpaca.json\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\",\n",
    "            \"system\": \"system\",\n",
    "            \"history\": \"history\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"data/dataset_info.json\", \"w\") as f:\n",
    "        json.dump(datasets, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c54b0c37-d8e8-4dca-bf9e-fa7306d73f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run in terminal on the same node this OnDemand session is on.\n",
    "# llamafactory-cli webui # by default it should launch a Gradio board on localhost:7860\n",
    "## Run locally\n",
    "# ssh -L <port shown above>:localhost:<port shown above> local_port_forwarding_test\n",
    "## Open a browser locally to interface with Gradio board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70c4e87e-78ea-4862-b6b4-bd1ec840db55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nllamafactory-cli train     --stage sft     --do_train True     --model_name_or_path meta-llama/Llama-3.2-90B-Vision-Instruct     --preprocessing_num_workers 16     --finetuning_type lora     --template mllama     --flash_attn auto     --dataset_dir data     --dataset dragen_alpaca     --cutoff_len 2048     --learning_rate 5e-05     --num_train_epochs 7.0     --max_samples 10000     --per_device_train_batch_size 32     --gradient_accumulation_steps 2     --lr_scheduler_type cosine     --max_grad_norm 1.0     --logging_steps 20     --save_steps 750     --warmup_steps 75     --packing False     --report_to none     --output_dir saves/Llama-3.2-90B-Vision-Instruct/lora/train_2024-12-06-13-08-42     --bf16 True     --plot_loss True     --ddp_timeout 180000000     --optim adamw_torch     --lora_rank 8     --lora_alpha 16     --lora_dropout 0.1     --loraplus_lr_ratio 0.1     --lora_target all     --val_size 0.1     --eval_strategy steps     --eval_steps 750     --per_device_eval_batch_size 32\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on Gradio board with\n",
    "'''\n",
    "llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path meta-llama/Llama-3.2-90B-Vision-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template mllama \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset dragen_alpaca \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 7.0 \\\n",
    "    --max_samples 10000 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 20 \\\n",
    "    --save_steps 750 \\\n",
    "    --warmup_steps 75 \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Llama-3.2-90B-Vision-Instruct/lora/train_2024-12-06-13-08-42 \\\n",
    "    --bf16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --optim adamw_torch \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --loraplus_lr_ratio 0.1 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 750 \\\n",
    "    --per_device_eval_batch_size 32\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27565857-a18d-4ead-b189-356bbe96cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources:\n",
    "# https://www.reddit.com/r/LocalLLaMA/comments/1fps1cp/llama32_vs_llama31_in_medical_domain_llama31_70b/\n",
    "# https://github.com/NVIDIA/RULER?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fad1f-859d-4555-86cd-0982abb175c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d7426-0f28-40a5-ac53-8d1d37af91ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
